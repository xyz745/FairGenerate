{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33fd0c2c-ded0-4841-8c93-44520fa8c1ed",
   "metadata": {},
   "source": [
    "This notebook presents the implementation of FairGenerate.\n",
    "\n",
    "**\"FairGenerate: Enhancing Fairness Through Synthetic Data Generation and Two-Fold Biased Label Removal \"**\n",
    "\n",
    "FairGenerate is a novel preprocessing method designed to mitigate imbalanced data and biased labels in training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c45c48-51dc-4359-acee-5b2f9891eb73",
   "metadata": {},
   "source": [
    "If you use this work, please cite our TOSEM 2025 paper:\n",
    "\n",
    "@article{10.1145/3730579,\n",
    "author = {Joshi, Hem Chandra and Kumar, Sandeep},\n",
    "title = {FairGenerate: Enhancing Fairness Through Synthetic Data Generation and Two-Fold Biased Labels Removal},\n",
    "year = {2025},\n",
    "publisher = {Association for Computing Machinery},\n",
    "address = {New York, NY, USA},\n",
    "issn = {1049-331X},\n",
    "url = {https://doi.org/10.1145/3730579},\n",
    "doi = {10.1145/3730579},\n",
    "note = {Just Accepted},\n",
    "journal = {ACM Trans. Softw. Eng. Methodol.},\n",
    "month = apr,\n",
    "keywords = {ML software, Software fairness, Bias mitigation, Imbalanced Data, Biased Labels}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1752ef30-ac1b-42c3-a21f-cbf3bf9c5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Learner\n",
    "from sklearn.neighbors import NearestNeighbors as NN\n",
    "\n",
    "#Function to find the K nearest neighhours\n",
    "def get_ngbr(df, knn):\n",
    "    #np.random.seed(0)\n",
    "    rand_sample_idx = random.randint(0, df.shape[0] - 1)\n",
    "    parent_candidate = df.iloc[rand_sample_idx]\n",
    "    distance,ngbr = knn.kneighbors(parent_candidate.values.reshape(1,-1),3,return_distance=True)    \n",
    "    candidate_1 = df.iloc[ngbr[0][1]]    \n",
    "    candidate_2 = df.iloc[ngbr[0][2]]    \n",
    "    return distance,parent_candidate,candidate_1,candidate_2\n",
    "\n",
    "\n",
    "def generate_samples(no_of_samples,df,df_name,protected_attribute):\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    #Calling function to find the KNN\n",
    "    total_data = df.values.tolist()\n",
    "    knn = NN(n_neighbors=5,algorithm='auto').fit(df)\n",
    "\n",
    "    column_name=df.columns.tolist()\n",
    "    #added by own\n",
    "    #new_candidate_df=pd.DataFrame(columns=column_name)\n",
    "    #added by own  end\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------\n",
    "    #Logic to create synthetic data\n",
    "\n",
    "    for _ in range(no_of_samples):\n",
    "    \n",
    "        f = .3\n",
    "        distance,parent_candidate, child_candidate_1, child_candidate_2 = get_ngbr(df, knn)      \n",
    "        mutant = []\n",
    "        for key,value in parent_candidate.items():        \n",
    "            #x1=distance[0][0]  \n",
    "            x1=distance[0][1] \n",
    "            x2=distance[0][2] \n",
    "            x3=abs(x2-x1)\n",
    "                \n",
    "            if isinstance(parent_candidate[key], (bool, str)):\n",
    "                if x1 <=x3:\n",
    "                    mutant.append(np.random.choice([parent_candidate[key], child_candidate_1[key]]))   \n",
    "                else:\n",
    "                    mutant.append(np.random.choice([child_candidate_1[key], child_candidate_2[key]]))                      \n",
    "            else:             \n",
    "                if x1 <= x3:\n",
    "                    mutant.append(parent_candidate[key] + f * (child_candidate_1[key] - parent_candidate[key]))\n",
    "                else:\n",
    "                    mutant.append(abs(child_candidate_1[key] + f * (child_candidate_2[key] - child_candidate_1[key])))\n",
    "        total_data.append(mutant)\n",
    "   \n",
    "    final_df = pd.DataFrame(total_data)\n",
    "    #--------------------------------------------------------------------------------------------------------------\n",
    "    #Rename dataframe columns\n",
    "    final_df=final_df.set_axis(column_name, axis=1)\n",
    "\n",
    "    return final_df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9c3b84-4174-464c-acaa-c4c01cfa29f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "#Importing Library\n",
    "\n",
    "# %%\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# %%\n",
    "#Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Fairness Metrics\n",
    "from aif360.datasets import BinaryLabelDataset, StructuredDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "import time\n",
    "import calendar\n",
    "import copy\n",
    "\n",
    "#To split-training and testing dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd127fc8-3137-436c-b9c6-c83fdb346fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e71092-dd45-47a7-800c-8c16bd544e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Situation Testing Code\n",
    "\n",
    "def situation(clf,X_train,y_train,keyword):\n",
    "    #they have used  as a classifier\n",
    "    X_flip = X_train.copy()\n",
    "    X_flip[keyword] = np.where(X_flip[keyword]==1, 0, 1)\n",
    "    a = np.array(clf.predict(X_train))\n",
    "    b = np.array(clf.predict(X_flip))\n",
    "    same = (a==b)\n",
    "    #print(same) #[True  True  True ... False  True  True  True]\n",
    "    same = [1 if each else 0 for each in same]  #[1 1 1... 0 1 1 1] if true makes it 1 ,else 0\n",
    "    X_train['same'] = same #make a new column 'same' and put above list into it.\n",
    "    X_train['y'] = y_train #make a new column 'y' and put y_train value into it.\n",
    "    X_rest = X_train[X_train['same']==1] #This creates a new DataFrame (X_rest) that contains only the rows where the 'same' column is 1.\n",
    "    y_rest = X_rest['y']\n",
    "    X_rest = X_rest.drop(columns=['same','y'])\n",
    "\n",
    "    print(\"Removed Points:\",np.round((X_train.shape[0] - X_rest.shape[0]) / X_train.shape[0] * 100, 4),\"% || \", X_train.shape[0]-X_rest.shape[0])\n",
    "    point_removed=np.round((X_train.shape[0] - X_rest.shape[0]) / X_train.shape[0] * 100, 4)\n",
    "    \n",
    "    return X_rest,y_rest,point_removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece3ab7c-c00b-4f86-a708-5f96292cb627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: 1 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 14.157 % ||  817\n",
      "Before Synthetic Data Generation\n",
      "1849 341 2134 630\n",
      "After Synthetic Data Generation\n",
      "2134 2134 2134 2134\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 0.0 % ||  0\n",
      "\n",
      "Running: 2 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 14.2783 % ||  824\n",
      "Before Synthetic Data Generation\n",
      "1846 337 2128 636\n",
      "After Synthetic Data Generation\n",
      "2128 2128 2128 2128\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 6.438 % ||  548\n",
      "\n",
      "Running: 3 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 17.7266 % ||  1023\n",
      "Before Synthetic Data Generation\n",
      "1754 308 2055 631\n",
      "After Synthetic Data Generation\n",
      "2055 2055 2055 2055\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 1.0706 % ||  88\n",
      "\n",
      "Running: 4 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 18.1771 % ||  1049\n",
      "Before Synthetic Data Generation\n",
      "1826 304 2000 592\n",
      "After Synthetic Data Generation\n",
      "2000 2000 2000 2000\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 0.0 % ||  0\n",
      "\n",
      "Running: 5 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 21.8333 % ||  1260\n",
      "Before Synthetic Data Generation\n",
      "1687 274 1949 601\n",
      "After Synthetic Data Generation\n",
      "1949 1949 1949 1949\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 5.567 % ||  434\n",
      "\n",
      "Running: 6 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 18.4717 % ||  1066\n",
      "Before Synthetic Data Generation\n",
      "1796 300 2005 604\n",
      "After Synthetic Data Generation\n",
      "2005 2005 2005 2005\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 5.1496 % ||  413\n",
      "\n",
      "Running: 7 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 17.8652 % ||  1031\n",
      "Before Synthetic Data Generation\n",
      "1761 300 2056 623\n",
      "After Synthetic Data Generation\n",
      "2056 2056 2056 2056\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 1.216 % ||  100\n",
      "\n",
      "Running: 8 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 13.4985 % ||  779\n",
      "Before Synthetic Data Generation\n",
      "1862 344 2133 653\n",
      "After Synthetic Data Generation\n",
      "2133 2133 2133 2133\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 11.6034 % ||  990\n",
      "\n",
      "Running: 9 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 18.1078 % ||  1045\n",
      "Before Synthetic Data Generation\n",
      "1812 311 1986 617\n",
      "After Synthetic Data Generation\n",
      "1986 1986 1986 1986\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 3.7135 % ||  295\n",
      "\n",
      "Running: 10 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 18.7489 % ||  1082\n",
      "Before Synthetic Data Generation\n",
      "1811 300 1990 588\n",
      "After Synthetic Data Generation\n",
      "1990 1990 1990 1990\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 3.9698 % ||  316\n",
      "\n",
      "Running: 11 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 18.5756 % ||  1072\n",
      "Before Synthetic Data Generation\n",
      "1782 310 1983 624\n",
      "After Synthetic Data Generation\n",
      "1983 1983 1983 1983\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 3.6939 % ||  293\n",
      "\n",
      "Running: 12 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 13.8451 % ||  799\n",
      "Before Synthetic Data Generation\n",
      "1826 340 2170 636\n",
      "After Synthetic Data Generation\n",
      "2170 2170 2170 2170\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 6.2327 % ||  541\n",
      "\n",
      "Running: 13 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 18.8182 % ||  1086\n",
      "Before Synthetic Data Generation\n",
      "1786 303 2007 589\n",
      "After Synthetic Data Generation\n",
      "2007 2007 2007 2007\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 0.0 % ||  0\n",
      "\n",
      "Running: 14 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 17.6226 % ||  1017\n",
      "Before Synthetic Data Generation\n",
      "1758 309 2059 628\n",
      "After Synthetic Data Generation\n",
      "2059 2059 2059 2059\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 1.1535 % ||  95\n",
      "\n",
      "Running: 15 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 22.3878 % ||  1292\n",
      "Before Synthetic Data Generation\n",
      "1670 263 1930 616\n",
      "After Synthetic Data Generation\n",
      "1930 1930 1930 1930\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 1.114 % ||  86\n",
      "\n",
      "Running: 16 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 22.1972 % ||  1281\n",
      "Before Synthetic Data Generation\n",
      "1668 274 1922 626\n",
      "After Synthetic Data Generation\n",
      "1922 1922 1922 1922\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 1.3658 % ||  105\n",
      "\n",
      "Running: 17 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 13.9837 % ||  807\n",
      "Before Synthetic Data Generation\n",
      "1866 350 2107 641\n",
      "After Synthetic Data Generation\n",
      "2107 2107 2107 2107\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 9.8007 % ||  826\n",
      "\n",
      "Running: 18 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 22.1452 % ||  1278\n",
      "Before Synthetic Data Generation\n",
      "1680 285 1929 599\n",
      "After Synthetic Data Generation\n",
      "1929 1929 1929 1929\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 1.296 % ||  100\n",
      "\n",
      "Running: 19 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 18.6623 % ||  1077\n",
      "Before Synthetic Data Generation\n",
      "1792 308 1978 616\n",
      "After Synthetic Data Generation\n",
      "1978 1978 1978 1978\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 3.7791 % ||  299\n",
      "\n",
      "Running: 20 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 22.2145 % ||  1282\n",
      "Before Synthetic Data Generation\n",
      "1683 271 1924 611\n",
      "After Synthetic Data Generation\n",
      "1924 1924 1924 1924\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 5.1455 % ||  396\n",
      "                0          1         2         3       4       5       6  \\\n",
      "Metric     Recall  Precision  Accuracy  F1 Score     AOD     EOD     SPD   \n",
      "Original   0.7036      0.664    0.6452    0.6867  0.2512  0.2245  0.2726   \n",
      "Processed  0.5502     0.7081    0.6286    0.6165  0.0421  0.0523  0.0274   \n",
      "\n",
      "                7  \n",
      "Metric         DI  \n",
      "Original   0.3448  \n",
      "Processed  0.0635  \n",
      "\n",
      "Running: 1 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 10.2235 % ||  590\n",
      "Before Synthetic Data Generation\n",
      "1662 691 1759 1069\n",
      "After Synthetic Data Generation\n",
      "1759 1759 1759 1759\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 5.3297 % ||  375\n",
      "\n",
      "Running: 2 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 10.1022 % ||  583\n",
      "Before Synthetic Data Generation\n",
      "1657 698 1753 1080\n",
      "After Synthetic Data Generation\n",
      "1753 1753 1753 1753\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 0.0 % ||  0\n",
      "\n",
      "Running: 3 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 12.5455 % ||  724\n",
      "Before Synthetic Data Generation\n",
      "1618 653 1717 1059\n",
      "After Synthetic Data Generation\n",
      "1717 1717 1717 1717\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 5.2563 % ||  361\n",
      "\n",
      "Running: 4 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 0.0 % ||  0\n",
      "Before Synthetic Data Generation\n",
      "1840 795 1976 1160\n",
      "After Synthetic Data Generation\n",
      "1976 1976 1976 1976\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 0.0 % ||  0\n",
      "\n",
      "Running: 5 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 1.9754 % ||  114\n",
      "Before Synthetic Data Generation\n",
      "1770 745 1972 1170\n",
      "After Synthetic Data Generation\n",
      "1972 1972 1972 1972\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 0.0 % ||  0\n",
      "\n",
      "Running: 6 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 0.0 % ||  0\n",
      "Before Synthetic Data Generation\n",
      "1835 771 1972 1193\n",
      "After Synthetic Data Generation\n",
      "1972 1972 1972 1972\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 0.0 % ||  0\n",
      "\n",
      "Running: 7 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 12.6321 % ||  729\n",
      "Before Synthetic Data Generation\n",
      "1611 659 1721 1051\n",
      "After Synthetic Data Generation\n",
      "1721 1721 1721 1721\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 0.0 % ||  0\n",
      "\n",
      "Running: 8 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 9.7557 % ||  563\n",
      "Before Synthetic Data Generation\n",
      "1678 694 1755 1081\n",
      "After Synthetic Data Generation\n",
      "1755 1755 1755 1755\n",
      "Situation Testing After Synthetic Data Generation......\n",
      "Removed Points: 5.6695 % ||  398\n",
      "\n",
      "Running: 9 ---------\n",
      "(5771, 6)\n",
      "(1443, 6)\n",
      "Situation Testing Before Synthetic Data Generation......\n",
      "Removed Points: 0.0 % ||  0\n",
      "Before Synthetic Data Generation\n",
      "1839 784 1960 1188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 178\u001b[0m\n\u001b[1;32m    176\u001b[0m df_zero_zero \u001b[38;5;241m=\u001b[39m generate_samples(zero_zero_to_be_increased,df_zero_zero,dataset_name, protected_attribute)\n\u001b[1;32m    177\u001b[0m df_zero_one \u001b[38;5;241m=\u001b[39m generate_samples(zero_one_to_be_increased,df_zero_one,dataset_name, protected_attribute)\n\u001b[0;32m--> 178\u001b[0m df_one_one \u001b[38;5;241m=\u001b[39m generate_samples(one_one_to_be_increased,df_one_one,dataset_name, protected_attribute)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m#appending dataframes\u001b[39;00m\n\u001b[1;32m    181\u001b[0m df \u001b[38;5;241m=\u001b[39m df_zero_one\u001b[38;5;241m.\u001b[39mappend(df_zero_zero)\n",
      "Cell \u001b[0;32mIn[1], line 37\u001b[0m, in \u001b[0;36mgenerate_samples\u001b[0;34m(no_of_samples, df, df_name, protected_attribute)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_of_samples):\n\u001b[1;32m     36\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.3\u001b[39m\n\u001b[0;32m---> 37\u001b[0m     distance,parent_candidate, child_candidate_1, child_candidate_2 \u001b[38;5;241m=\u001b[39m get_ngbr(df, knn)      \n\u001b[1;32m     38\u001b[0m     mutant \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m parent_candidate\u001b[38;5;241m.\u001b[39mitems():        \n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m#x1=distance[0][0]  \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m, in \u001b[0;36mget_ngbr\u001b[0;34m(df, knn)\u001b[0m\n\u001b[1;32m     11\u001b[0m rand_sample_idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m parent_candidate \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[rand_sample_idx]\n\u001b[0;32m---> 13\u001b[0m distance,ngbr \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mkneighbors(parent_candidate\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\u001b[38;5;241m3\u001b[39m,return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)    \n\u001b[1;32m     14\u001b[0m candidate_1 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[ngbr[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]]    \n\u001b[1;32m     15\u001b[0m candidate_2 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[ngbr[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]]    \n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_base.py:879\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    874\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    875\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not work with sparse matrices. Densify the data, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    876\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor set algorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    877\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method\n\u001b[1;32m    878\u001b[0m         )\n\u001b[0;32m--> 879\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs, prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[1;32m    880\u001b[0m         delayed(_tree_query_parallel_helper)(\n\u001b[1;32m    881\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree, X[s], n_neighbors, return_distance\n\u001b[1;32m    882\u001b[0m         )\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m gen_even_slices(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_jobs)\n\u001b[1;32m    884\u001b[0m     )\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal: _fit_method not recognized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1341\u001b[0m, in \u001b[0;36mParallel.__init__\u001b[0;34m(self, n_jobs, backend, return_as, verbose, timeout, pre_dispatch, batch_size, temp_folder, max_nbytes, mmap_mode, prefer, require)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;241m=\u001b[39m uuid4()\u001b[38;5;241m.\u001b[39mhex\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_ref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/uuid.py:723\u001b[0m, in \u001b[0;36muuid4\u001b[0;34m()\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21muuid4\u001b[39m():\n\u001b[1;32m    722\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate a random UUID.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UUID(\u001b[38;5;28mbytes\u001b[39m\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39murandom(\u001b[38;5;241m16\u001b[39m), version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/uuid.py:222\u001b[0m, in \u001b[0;36mUUID.__init__\u001b[0;34m(self, hex, bytes, bytes_le, fields, int, version, is_safe)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m(\u001b[38;5;241m0xf000\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m version \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m76\u001b[39m\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_safe\u001b[39m\u001b[38;5;124m'\u001b[39m, is_safe)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_names=['Compas','Compas','German','Heart','Student','Bank','Meps15','Meps16','Default','Adult','Adult']\n",
    "protected_attributes=['sex','race','sex','age','sex','age','race','race','sex','sex','race']\n",
    "\n",
    "for dataset_index in range(0,len(dataset_names)):\n",
    "\n",
    "    dataset_name= dataset_names[dataset_index].lower()\n",
    "    protected_attribute=protected_attributes[dataset_index]\n",
    "\n",
    "    folder_name=dataset_name.lower()+\"_\"+ protected_attribute\n",
    "    \n",
    "    test_size_input=0.20\n",
    "    shuffle_value=True\n",
    "    learner='LGR'\n",
    "\n",
    "    original_accuracy,original_recall,original_f1score,original_precision=[],[],[],[]\n",
    "    original_aod,original_eod,original_spd,original_di=[],[],[],[]\n",
    "    \n",
    "    processed_accuracy,processed_recall,processed_f1score,processed_precision=[],[],[],[]\n",
    "    processed_aod,processed_eod,processed_spd,processed_di=[],[],[],[]\n",
    "\n",
    "    for random_seed in range(1,21):  \n",
    "\n",
    "        store_value={}\n",
    "       \n",
    "        current_GMT = time.gmtime()\n",
    "        global_timestamp = calendar.timegm(current_GMT)\n",
    "        \n",
    "        os.makedirs(f\"generated_data/{folder_name}/{global_timestamp}\", exist_ok=True)\n",
    "        \n",
    "        print(\"\\nRunning:\",random_seed,\"---------\")\n",
    "        #Import dataset\n",
    "        dataset_orig=pd.read_csv(\"preprocessed_dataset/\"+dataset_name+\"_processed.csv\")\n",
    "        \n",
    "        #Normalize dataset\n",
    "        scaler = MinMaxScaler()\n",
    "        dataset_orig = pd.DataFrame(scaler.fit_transform(dataset_orig),columns = dataset_orig.columns)\n",
    "        #dataset_orig\n",
    "\n",
    "\n",
    "        #Split dataset into training and test set\n",
    "        dataset_orig_train, dataset_orig_test = train_test_split(dataset_orig, test_size=test_size_input, shuffle=shuffle_value, random_state=random_seed)\n",
    "        print(dataset_orig_train.shape)\n",
    "        print(dataset_orig_test.shape)\n",
    "\n",
    "\n",
    "        #Train Set\n",
    "        X_train , y_train = dataset_orig_train.loc[:, dataset_orig_train.columns != 'Probability'],\\\n",
    "        dataset_orig_train['Probability']\n",
    "        \n",
    "        #Test Set\n",
    "        X_test , y_test = dataset_orig_test.loc[:, dataset_orig_test.columns != 'Probability'],\\\n",
    "        dataset_orig_test['Probability']\n",
    "\n",
    "        ##========================Without any bias mitigation techniques==================\n",
    "        \n",
    "        clf_lr = LogisticRegression(random_state=random_seed)\n",
    "        clf_lr.fit(X_train,y_train)\n",
    "        \n",
    "        # Prepare dataset_t for metrics calculation\n",
    "        dataset_t = BinaryLabelDataset(favorable_label=1.0,\n",
    "                                       unfavorable_label=0.0,\n",
    "                                       df=dataset_orig_test,\n",
    "                                       label_names=['Probability'],\n",
    "                                       protected_attribute_names=[protected_attribute])\n",
    "        \n",
    "        y_pred = clf_lr.predict(X_test)\n",
    "            \n",
    "        dataset_pred = dataset_t.copy()  \n",
    "        dataset_pred.labels = y_pred    \n",
    "        attr = dataset_t.protected_attribute_names[0]\n",
    "        idx = dataset_t.protected_attribute_names.index(attr)\n",
    "        \n",
    "        privileged_groups = [{attr: dataset_pred.privileged_protected_attributes[idx][0]}]\n",
    "        unprivileged_groups = [{attr: dataset_pred.unprivileged_protected_attributes[idx][0]}]\n",
    "        \n",
    "        class_metrics = ClassificationMetric(dataset_t, dataset_pred, unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "        original_accuracy.append(class_metrics.accuracy())\n",
    "        original_recall.append(class_metrics.recall())\n",
    "        original_precision.append(class_metrics.precision())\n",
    "        original_f1score.append((2 * class_metrics.recall() * class_metrics.precision()) / (class_metrics.precision() + class_metrics.recall()))\n",
    "        \n",
    "        original_aod.append(np.abs(class_metrics.average_odds_difference()))\n",
    "        original_eod.append(np.abs(class_metrics.equal_opportunity_difference()))\n",
    "        original_spd.append(np.abs(class_metrics.statistical_parity_difference()))\n",
    "        original_di.append(np.abs(1 - class_metrics.disparate_impact()))\n",
    "\n",
    "        #================= FairGenerate Applies Below ===============\n",
    "\n",
    "        #Step 1 - Data Cleaning.\n",
    "\n",
    "\n",
    "        clf1 = LogisticRegression(random_state=random_seed)\n",
    "        clf1.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Situation Testing Before Synthetic Data Generation......\")\n",
    "        X_train, y_train,point_removed = situation(clf1, X_train, y_train, protected_attribute) #dataset is changing. \n",
    "    \n",
    "        #For Further Situation Testing\n",
    "        clf2 = LogisticRegression(random_state=random_seed)\n",
    "        clf2.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "        dataset_orig_train=X_train\n",
    "        dataset_orig_train['Probability']=y_train\n",
    "\n",
    "        #Step 2 - Data Balancing\n",
    "           \n",
    "        # first one is class value and second one is protected attribute value\n",
    "        zero_zero = len(dataset_orig_train[(dataset_orig_train['Probability'] == 0) & (dataset_orig_train[protected_attribute] == 0)])\n",
    "        zero_one = len(dataset_orig_train[(dataset_orig_train['Probability'] == 0) & (dataset_orig_train[protected_attribute] == 1)])\n",
    "        one_zero = len(dataset_orig_train[(dataset_orig_train['Probability'] == 1) & (dataset_orig_train[protected_attribute] == 0)])\n",
    "        one_one = len(dataset_orig_train[(dataset_orig_train['Probability'] == 1) & (dataset_orig_train[protected_attribute] == 1)])\n",
    "\n",
    "        print(\"Before Synthetic Data Generation\")\n",
    "\n",
    "        print(zero_zero,zero_one,one_zero,one_one)\n",
    "    \n",
    "        maximum = max(zero_zero,zero_one,one_zero,one_one)\n",
    "             \n",
    "        zero_zero_to_be_increased = maximum - zero_zero ## where class is 0 attribute is 0\n",
    "        zero_one_to_be_increased = maximum - zero_one ## where class is 0 attribute is 1\n",
    "        one_zero_to_be_increased = maximum - one_zero ## where class is 1 attribute is 0\n",
    "        one_one_to_be_increased = maximum - one_one ## where class is 1 attribute is 1\n",
    "        \n",
    "        df_zero_zero = dataset_orig_train[(dataset_orig_train['Probability'] == 0) & (dataset_orig_train[protected_attribute] == 0)]\n",
    "        df_one_one = dataset_orig_train[(dataset_orig_train['Probability'] == 1) & (dataset_orig_train[protected_attribute] == 1)]\n",
    "        df_one_zero = dataset_orig_train[(dataset_orig_train['Probability'] == 1) & (dataset_orig_train[protected_attribute] == 0)]\n",
    "        df_zero_one = dataset_orig_train[(dataset_orig_train['Probability'] == 0) & (dataset_orig_train[protected_attribute] == 1)]\n",
    "    \n",
    "      \n",
    "        if zero_zero_to_be_increased==0:\n",
    "            \n",
    "            df_zero_one[protected_attribute] = df_zero_one[protected_attribute].astype(str)\n",
    "            df_one_zero[protected_attribute] = df_one_zero[protected_attribute].astype(str)\n",
    "            df_one_one[protected_attribute] = df_one_one[protected_attribute].astype(str)\n",
    "    \n",
    "            #calling generate_samples\n",
    "            df_zero_one = generate_samples(zero_one_to_be_increased,df_zero_one,dataset_name, protected_attribute)\n",
    "            df_one_zero = generate_samples(one_zero_to_be_increased,df_one_zero,dataset_name, protected_attribute)\n",
    "            df_one_one = generate_samples(one_one_to_be_increased,df_one_one,dataset_name, protected_attribute)\n",
    "            \n",
    "            #appending dataframes\n",
    "            df = df_one_zero.append(df_zero_one)\n",
    "            df = df.append(df_one_one)\n",
    "            df[protected_attribute]=df[protected_attribute].astype(float)\n",
    "            df_zero_zero = dataset_orig_train[(dataset_orig_train['Probability'] == 0) & (dataset_orig_train[protected_attribute] == 0)]\n",
    "            df = df.append(df_zero_zero)\n",
    "    \n",
    "        \n",
    "        elif zero_one_to_be_increased==0:\n",
    "    \n",
    "            df_zero_zero[protected_attribute] = df_zero_zero[protected_attribute].astype(str)\n",
    "            df_one_zero[protected_attribute] = df_one_zero[protected_attribute].astype(str)\n",
    "            df_one_one[protected_attribute] = df_one_one[protected_attribute].astype(str)\n",
    "      \n",
    "            #calling generate_samples\n",
    "            df_zero_zero = generate_samples(zero_zero_to_be_increased,df_zero_zero,dataset_name, protected_attribute)\n",
    "            df_one_zero = generate_samples(one_zero_to_be_increased,df_one_zero,dataset_name, protected_attribute)\n",
    "            df_one_one = generate_samples(one_one_to_be_increased,df_one_one,dataset_name, protected_attribute)\n",
    "    \n",
    "            #appending dataframes\n",
    "            df = df_one_zero.append(df_zero_zero)\n",
    "            df = df.append(df_one_one)\n",
    "            df[protected_attribute]= df[protected_attribute].astype(float)\n",
    "            df_zero_one = dataset_orig_train[(dataset_orig_train['Probability'] == 0) & (dataset_orig_train[protected_attribute] == 1)]\n",
    "            df = df.append(df_zero_one)\n",
    "        \n",
    "        elif one_zero_to_be_increased==0:\n",
    "            df_zero_one[protected_attribute] = df_zero_one[protected_attribute].astype(str)\n",
    "            df_zero_zero[protected_attribute] = df_zero_zero[protected_attribute].astype(str)\n",
    "            df_one_one[protected_attribute] = df_one_one[protected_attribute].astype(str)\n",
    "    \n",
    "            #calling generate_samples\n",
    "            df_zero_zero = generate_samples(zero_zero_to_be_increased,df_zero_zero,dataset_name, protected_attribute)\n",
    "            df_zero_one = generate_samples(zero_one_to_be_increased,df_zero_one,dataset_name, protected_attribute)\n",
    "            df_one_one = generate_samples(one_one_to_be_increased,df_one_one,dataset_name, protected_attribute)\n",
    "    \n",
    "            #appending dataframes\n",
    "            df = df_zero_one.append(df_zero_zero)\n",
    "            df = df.append(df_one_one)\n",
    "            df[protected_attribute] = df[protected_attribute].astype(float)\n",
    "            df_one_zero = dataset_orig_train[(dataset_orig_train['Probability'] == 1) & (dataset_orig_train[protected_attribute] == 0)]\n",
    "            df = df.append(df_one_zero)\n",
    "        \n",
    "        elif one_one_to_be_increased==0:\n",
    "    \n",
    "            df_zero_zero[protected_attribute] = df_zero_zero[protected_attribute].astype(str)\n",
    "            df_zero_one[protected_attribute] = df_zero_one[protected_attribute].astype(str)\n",
    "            df_one_zero[protected_attribute] = df_one_zero[protected_attribute].astype(str)\n",
    "               \n",
    "            #calling generate_samples\n",
    "            df_zero_zero = generate_samples(zero_zero_to_be_increased,df_zero_zero,dataset_name, protected_attribute)\n",
    "            df_zero_one = generate_samples(zero_one_to_be_increased,df_zero_one,dataset_name, protected_attribute)\n",
    "            df_one_zero = generate_samples(one_zero_to_be_increased,df_one_zero,dataset_name, protected_attribute)\n",
    "            \n",
    "            #appending dataframes\n",
    "            df = df_zero_one.append(df_zero_zero)\n",
    "            df = df.append(df_one_zero)\n",
    "            df[protected_attribute] = df[protected_attribute].astype(float)\n",
    "            df_one_one = dataset_orig_train[(dataset_orig_train['Probability'] == 1) & (dataset_orig_train[protected_attribute] == 1)]\n",
    "            df = df.append(df_one_one)\n",
    "    \n",
    "        # first one is class value and second one is protected attribute value\n",
    "        zero_zero = len(df[(df['Probability'] == 0) & (df[protected_attribute] == 0)])\n",
    "        zero_one = len(df[(df['Probability'] == 0) & (df[protected_attribute] == 1)])\n",
    "        one_zero = len(df[(df['Probability'] == 1) & (df[protected_attribute] == 0)])\n",
    "        one_one = len(df[(df['Probability'] == 1) & (df[protected_attribute] == 1)])\n",
    "\n",
    "        print(\"After Synthetic Data Generation\")\n",
    "        print(zero_zero,zero_one,one_zero,one_one)\n",
    "        \n",
    "        X_train, y_train = df.loc[:, df.columns != 'Probability'], df['Probability']\n",
    "    \n",
    "        print(\"Situation Testing After Synthetic Data Generation......\")\n",
    "\n",
    "        #---------------- Step 3 : Fair-Situation Testing -----------------\n",
    "        \n",
    "        X_train, y_train, point_removed = situation(clf2, X_train, y_train, protected_attribute) #dataset is changing. \n",
    "\n",
    "        df=copy.deepcopy(X_train)\n",
    "        df['Probability']=y_train\n",
    "\n",
    "        \n",
    "        clf_lr = LogisticRegression(random_state=random_seed)\n",
    "        clf_lr.fit(X_train,y_train)\n",
    "        \n",
    "        # Prepare dataset_t for metrics calculation\n",
    "        dataset_t = BinaryLabelDataset(favorable_label=1.0,\n",
    "                                       unfavorable_label=0.0,\n",
    "                                       df=dataset_orig_test,\n",
    "                                       label_names=['Probability'],\n",
    "                                       protected_attribute_names=[protected_attribute])\n",
    "        \n",
    "        y_pred = clf_lr.predict(X_test)\n",
    "            \n",
    "        dataset_pred = dataset_t.copy()  \n",
    "        dataset_pred.labels = y_pred    \n",
    "        attr = dataset_t.protected_attribute_names[0]\n",
    "        idx = dataset_t.protected_attribute_names.index(attr)\n",
    "        \n",
    "        privileged_groups = [{attr: dataset_pred.privileged_protected_attributes[idx][0]}]\n",
    "        unprivileged_groups = [{attr: dataset_pred.unprivileged_protected_attributes[idx][0]}]\n",
    "        \n",
    "        class_metrics = ClassificationMetric(dataset_t, dataset_pred, unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "        processed_accuracy.append(class_metrics.accuracy())\n",
    "        processed_recall.append(class_metrics.recall())\n",
    "        processed_precision.append(class_metrics.precision())\n",
    "        processed_f1score.append((2 * class_metrics.recall() * class_metrics.precision()) / (class_metrics.precision() + class_metrics.recall()))\n",
    "        \n",
    "        processed_aod.append(np.abs(class_metrics.average_odds_difference()))\n",
    "        processed_eod.append(np.abs(class_metrics.equal_opportunity_difference()))\n",
    "        processed_spd.append(np.abs(class_metrics.statistical_parity_difference()))\n",
    "        processed_di.append(np.abs(1 - class_metrics.disparate_impact()))\n",
    "    \n",
    "              \n",
    "        # Compute the metrics\n",
    "    metrics = {\n",
    "        \"Metric\": [\"Recall\", \"Precision\", \"Accuracy\", \"F1 Score\", \"AOD\", \"EOD\", \"SPD\", \"DI\"],\n",
    "        \"Original\": [\n",
    "            round(np.median(original_recall), 4),\n",
    "            round(np.median(original_precision), 4),\n",
    "            round(np.median(original_accuracy), 4),\n",
    "            round(np.median(original_f1score), 4),\n",
    "            round(np.median(original_aod), 4),\n",
    "            round(np.median(original_eod), 4),\n",
    "            round(np.median(original_spd), 4),\n",
    "            round(np.median(original_di), 4),\n",
    "           \n",
    "        ],\n",
    "        \"Processed\": [\n",
    "            round(np.median(processed_recall), 4),\n",
    "            round(np.median(processed_precision), 4),\n",
    "            round(np.median(processed_accuracy), 4),\n",
    "            round(np.median(processed_f1score), 4),\n",
    "            round(np.median(processed_aod), 4),\n",
    "            round(np.median(processed_eod), 4),\n",
    "            round(np.median(processed_spd), 4),\n",
    "            round(np.median(processed_di), 4),\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(metrics)\n",
    "    df=df.T\n",
    "    df.to_csv(\"Results_ST1_Synthetic_Data/fair_generate_\"+dataset_name+\" \"+protected_attribute+\" \"+learner+\".csv\",index=True)\n",
    "    # Display DataFrame\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406f0e3-b79b-4956-834c-e1d77079ec32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a2ec8-c6bb-451c-a440-494cefe61d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a4d26-531f-4ca9-b7f8-ef882f7a7172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
